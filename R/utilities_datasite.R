prepareDataSite = function(path) {
  require(BBmisc)
  require(mlr)
  tuple = load2(path)
  df = tuple$df
  targetname = tuple$targetname
  dataset_id = tuple$dataset_id
  dataset_names = unique(df[, dataset_id])
  list_dataset_index = lapply(1:length(dataset_names), function(i) which(df[, dataset_id] == dataset_names[i]))
  names(list_dataset_index) = dataset_names
  which(colnames(df) == dataset_id)
  df_dataset_accn = df[, dataset_id]
  df[dataset_id] = NULL   # FIXME: add args to allow deletion of other columns
  task = makeClassifTask(id = "holdoutHackTask", data = df, target = targetname)
  return(list(task = task, list_dataset_index = list_dataset_index, df_dataset_accn = df_dataset_accn))
}

# oml_task_id: 3891, 14966, 
getMlrTaskFromOML = function(oml_task_id) {
  require(OpenML)
  ot = OpenML::getOMLTask(oml_task_id)
  mt = OpenML::convertOMLTaskToMlr(ot)
  mlr_task = mt$mlr.task
  return(mlr_task)
}


clusterMlrTask = function(mlr_task, n_datasets = 5, balanced = T, pca_var_ratio = 0.7) {
  df = getTaskData(mlr_task, target.extra = TRUE)
  if (!balanced) {
    checkmate::assert(!"dataset_accn" %in% colnames(df))
    ctsk = makeClusterTask(id = "data_cluster", df$data)
    lrn = makeLearner("cluster.kmeans", centers = n_datasets)
    lrn = makePreprocWrapperCaret(lrn, thresh = pca_var_ratio)
    mod = train(lrn, ctsk)
    prds = predict(mod, ctsk)
    list_dataset_index = sapply(unique(prds$data$response), function(x) which(prds$data$response == x))
  } else {
    desc = mlr_task$task.desc
    pt = lapply(unique(df$target), function(x) {
      data = df$data[df$target == x, ]
      ctsk = makeClusterTask(id = "data_cluster", data)
      lrn = makeLearner("cluster.kmeans", centers = n_datasets)
      lrn = makePreprocWrapperCaret(lrn, thresh = pca_var_ratio)
      mod = train(lrn, ctsk)
      prds = predict(mod, ctsk)
      sapply(unique(prds$data$response), function(x) which(prds$data$response == x))
    })
    list_dataset_index = lapply(seq_len(length(pt[[1]])), function(x) c(pt[[1]][[x]], pt[[2]][[x]]))
  }
  # Out is a list of indices for each dataset
  names(list_dataset_index) = paste0("ds", 1:n_datasets)
  return(list_dataset_index)
}

#FIXME: remove OML dependencies to arbitrary mlr task
createClassBalancedDfCluster = function(oml_task_id = 14966, n_datasets = 5, balanced = TRUE, pca_var_ratio = 0.7, getTaskFun = getMlrTaskFromOML) {
  require(mlr)
  checkmate::assertIntegerish(n_datasets)
  checkmate::assertIntegerish(oml_task_id)
  checkmate::assertLogical(balanced)
  mlr_task = getTaskFun(oml_task_id)
  list_dataset_index = clusterMlrTask(mlr_task, n_datasets = 5, balanced = balanced, pca_var_ratio = pca_var_ratio)
  return(list(task = mlr_task, list_dataset_index = list_dataset_index))
}


create_rdata_cluster = function(pca_var_ratio, tids = c(3891, 14966, 34536), n_datasets = 5, balanced = T, path_regx = "../Data/temp/oml_%s_pca_%s_clustered_classbalanced_%s.RData", persist = T) {
  list.tuple = lapply(tids, function(tid) {
      lst = createClassBalancedDfCluster(tid, n_datasets, balanced, pca_var_ratio)
      dflst = lapply(seq_len(length(lst$list_dataset_index)),
        function(i) {
          tsk = subsetTask(lst$task,subset = lst$list_dataset_index[[i]])
          df = getTaskData(tsk)
          df$dataset_accn = paste0("ds", i)
          return(df)
      })
      data = do.call("rbind", dflst)
      filena = sprintf(path_regx, tid, as.character(pca_var_ratio), balanced)
      tname = getTaskTargetNames(lst$task)
      tuple = list(df = data, targetname = tname, dataset_id = "dataset_accn")
      df_dataset_accn = data[, tuple$dataset_id]
      if (persist) save(tuple,  file = filena)  # saving data on disk is always a good idea since openml is not robust to download
      lst$df_dataset_accn = df_dataset_accn
      return(lst)
  })
  list.tuple
}

# This function must be used inside the problem since this method is random, only running one time is not fair.
createRandomStratifPartition = function(taskid = 3891, nsplits = 5, getTaskFun = getMlrTaskFromOML) {
  task = getTaskFun(taskid)
  df = getTaskData(task)
  # Stratify
  desc = task$task.desc
  dfp = df[which(df[, desc$target] == desc$positive), ]
  dfn = df[which(df[, desc$target] == desc$negative), ]

  posi = sample(seq_len(nrow(dfp)))
  negi = sample(seq_len(nrow(dfn)))

  data_accnp = rep(seq_len(nsplits), length.out = nrow(dfp))
  data_accnn = rep(seq_len(nsplits), length.out = nrow(dfn))
  dfp$dataset_accn = data_accnp[posi]
  dfn$dataset_accn = data_accnn[negi]
  df = rbind(dfp, dfn)
  list_dataset_index = split(seq_len(nrow(df)), df$dataset_accn)
  df_dataset_accn = paste0("ds", df$dataset_accn)
  # list_dataset_index is a list of indices for each dataset.
  # df_dataset_accn    is a vector of which datasite an operation is from.
  return(list(task = task, list_dataset_index = list_dataset_index, df_dataset_accn = df_dataset_accn))
}
